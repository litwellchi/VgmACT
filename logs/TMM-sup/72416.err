W0422 22:24:15.221000 2430170 site-packages/torch/distributed/run.py:792] 
W0422 22:24:15.221000 2430170 site-packages/torch/distributed/run.py:792] *****************************************
W0422 22:24:15.221000 2430170 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0422 22:24:15.221000 2430170 site-packages/torch/distributed/run.py:792] *****************************************
2025-04-22 22:24:21.658183: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-04-22 22:24:21.699511: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-04-22 22:24:21.699563: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-04-22 22:24:21.701164: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-04-22 22:24:21.709235: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-22 22:24:22.086678: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-04-22 22:24:22.128520: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-04-22 22:24:22.128584: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-04-22 22:24:22.130198: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-04-22 22:24:22.138493: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-22 22:24:23.196940: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-04-22 22:24:23.664244: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2025-04-22 22:24:25.172293: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2025-04-22 22:24:25.538264: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
[rank1]:[W422 22:24:37.372685476 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank0]:[W422 22:24:37.715403070 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-04-22 22:25:36.089998: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-04-22 22:25:36.840074: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-04-22 22:25:37.900300: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-04-22 22:25:38.779814: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-04-22 22:25:39.518009: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
2025-04-22 22:25:40.523592: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
W0000 00:00:1745331944.960288 2430581 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: "CropAndResize" attr { key: "T" value { type: DT_FLOAT } } attr { key: "extrapolation_value" value { f: 0 } } attr { key: "method" value { s: "bilinear" } } inputs { dtype: DT_FLOAT shape { dim { size: 1 } dim { size: 224 } dim { size: 224 } dim { size: -4 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } } device { type: "CPU" vendor: "GenuineIntel" model: "111" frequency: 2000 num_cores: 60 environment { key: "cpu_instruction_set" value: "AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2" } environment { key: "eigen" value: "3.4.90" } l1_cache_size: 49152 l2_cache_size: 2097152 l3_cache_size: 110100480 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: -5 } dim { size: -6 } dim { size: -4 } } }
wandb: Currently logged in as: litwellchi to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.7
wandb: Run data is saved locally in /aifs4su/mmcode/worldm/videoact/VgmACT/0422V27_DiTB_freeze_reuseAct_128vgm4f_rt1_20250422_222413--image_aug/wandb/run-20250422_222545-ynjiq361
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 0422V27_DiTB_freeze_reuseAct_128vgm4f_rt1_20250422_222413--image_aug
wandb: â­ï¸ View project at https://wandb.ai/litwellchi/vgmact-rlbench10
wandb: ðŸš€ View run at https://wandb.ai/litwellchi/vgmact-rlbench10/runs/ynjiq361
=>> [Epoch 000] Global Step 000000 =>> LR :: 0.000000:   0%|          | 0/40000 [00:00<?, ?it/s]WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
W0000 00:00:1745331946.323902 2430580 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: "CropAndResize" attr { key: "T" value { type: DT_FLOAT } } attr { key: "extrapolation_value" value { f: 0 } } attr { key: "method" value { s: "bilinear" } } inputs { dtype: DT_FLOAT shape { dim { size: 1 } dim { size: 224 } dim { size: 224 } dim { size: -4 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } } device { type: "CPU" vendor: "GenuineIntel" model: "111" frequency: 2000 num_cores: 60 environment { key: "cpu_instruction_set" value: "AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2" } environment { key: "eigen" value: "3.4.90" } l1_cache_size: 49152 l2_cache_size: 2097152 l3_cache_size: 110100480 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: -5 } dim { size: -6 } dim { size: -4 } } }
=>> [Epoch 000] Global Step 000000 =>> LR :: 0.000000:   0%|          | 1/40000 [00:28<312:56:54, 28.17s/it]=>> [Epoch 001] Global Step 000001 =>> LR :: 0.000020 - Loss :: 0.6508:   0%|          | 1/40000 [00:28<312:56:54, 28.17s/it]=>> [Epoch 001] Global Step 000001 =>> LR :: 0.000020 - Loss :: 0.6508:   0%|          | 2/40000 [00:46<246:59:22, 22.23s/it]=>> [Epoch 002] Global Step 000002 =>> LR :: 0.000020 - Loss :: 0.3011:   0%|          | 2/40000 [00:46<246:59:22, 22.23s/it]=>> [Epoch 002] Global Step 000002 =>> LR :: 0.000020 - Loss :: 0.3011:   0%|          | 3/40000 [01:04<225:43:22, 20.32s/it]=>> [Epoch 003] Global Step 000003 =>> LR :: 0.000020 - Loss :: 0.2616:   0%|          | 3/40000 [01:04<225:43:22, 20.32s/it]=>> [Epoch 003] Global Step 000003 =>> LR :: 0.000020 - Loss :: 0.2616:   0%|          | 4/40000 [01:20<206:56:11, 18.63s/it]=>> [Epoch 004] Global Step 000004 =>> LR :: 0.000020 - Loss :: 0.2148:   0%|          | 4/40000 [01:20<206:56:11, 18.63s/it]=>> [Epoch 004] Global Step 000004 =>> LR :: 0.000020 - Loss :: 0.2148:   0%|          | 5/40000 [01:38<205:15:05, 18.47s/it]=>> [Epoch 005] Global Step 000005 =>> LR :: 0.000020 - Loss :: 0.2022:   0%|          | 5/40000 [01:38<205:15:05, 18.47s/it]=>> [Epoch 005] Global Step 000005 =>> LR :: 0.000020 - Loss :: 0.2022:   0%|          | 6/40000 [01:56<204:24:55, 18.40s/it]=>> [Epoch 006] Global Step 000006 =>> LR :: 0.000020 - Loss :: 0.2018:   0%|          | 6/40000 [01:56<204:24:55, 18.40s/it]=>> [Epoch 006] Global Step 000006 =>> LR :: 0.000020 - Loss :: 0.2018:   0%|          | 7/40000 [02:15<205:30:21, 18.50s/it]=>> [Epoch 007] Global Step 000007 =>> LR :: 0.000020 - Loss :: 0.1826:   0%|          | 7/40000 [02:15<205:30:21, 18.50s/it]=>> [Epoch 007] Global Step 000007 =>> LR :: 0.000020 - Loss :: 0.1826:   0%|          | 8/40000 [02:31<197:02:30, 17.74s/it]=>> [Epoch 008] Global Step 000008 =>> LR :: 0.000020 - Loss :: 0.1845:   0%|          | 8/40000 [02:31<197:02:30, 17.74s/it]=>> [Epoch 008] Global Step 000008 =>> LR :: 0.000020 - Loss :: 0.1845:   0%|          | 9/40000 [02:49<198:47:41, 17.90s/it]=>> [Epoch 009] Global Step 000009 =>> LR :: 0.000020 - Loss :: 0.1640:   0%|          | 9/40000 [02:49<198:47:41, 17.90s/it]=>> [Epoch 009] Global Step 000009 =>> LR :: 0.000020 - Loss :: 0.1640:   0%|          | 10/40000 [03:08<200:03:52, 18.01s/it]=>> [Epoch 010] Global Step 000010 =>> LR :: 0.000020 - Loss :: 0.1615:   0%|          | 10/40000 [03:08<200:03:52, 18.01s/it]=>> [Epoch 010] Global Step 000010 =>> LR :: 0.000020 - Loss :: 0.1615:   0%|          | 11/40000 [03:27<203:38:52, 18.33s/it]=>> [Epoch 011] Global Step 000011 =>> LR :: 0.000020 - Loss :: 0.1521:   0%|          | 11/40000 [03:27<203:38:52, 18.33s/it]=>> [Epoch 011] Global Step 000011 =>> LR :: 0.000020 - Loss :: 0.1521:   0%|          | 12/40000 [03:43<196:19:18, 17.67s/it]=>> [Epoch 012] Global Step 000012 =>> LR :: 0.000020 - Loss :: 0.1478:   0%|          | 12/40000 [03:43<196:19:18, 17.67s/it]=>> [Epoch 012] Global Step 000012 =>> LR :: 0.000020 - Loss :: 0.1478:   0%|          | 13/40000 [04:01<198:23:13, 17.86s/it]=>> [Epoch 013] Global Step 000013 =>> LR :: 0.000020 - Loss :: 0.1437:   0%|          | 13/40000 [04:01<198:23:13, 17.86s/it]=>> [Epoch 013] Global Step 000013 =>> LR :: 0.000020 - Loss :: 0.1437:   0%|          | 14/40000 [04:19<199:34:23, 17.97s/it]=>> [Epoch 014] Global Step 000014 =>> LR :: 0.000020 - Loss :: 0.1414:   0%|          | 14/40000 [04:19<199:34:23, 17.97s/it]=>> [Epoch 014] Global Step 000014 =>> LR :: 0.000020 - Loss :: 0.1414:   0%|          | 15/40000 [04:38<200:30:16, 18.05s/it]=>> [Epoch 015] Global Step 000015 =>> LR :: 0.000020 - Loss :: 0.1361:   0%|          | 15/40000 [04:38<200:30:16, 18.05s/it]=>> [Epoch 015] Global Step 000015 =>> LR :: 0.000020 - Loss :: 0.1361:   0%|          | 16/40000 [04:54<193:45:23, 17.45s/it]=>> [Epoch 016] Global Step 000016 =>> LR :: 0.000020 - Loss :: 0.1372:   0%|          | 16/40000 [04:54<193:45:23, 17.45s/it]=>> [Epoch 016] Global Step 000016 =>> LR :: 0.000020 - Loss :: 0.1372:   0%|          | 17/40000 [05:12<196:15:54, 17.67s/it]=>> [Epoch 017] Global Step 000017 =>> LR :: 0.000020 - Loss :: 0.1333:   0%|          | 17/40000 [05:12<196:15:54, 17.67s/it]=>> [Epoch 017] Global Step 000017 =>> LR :: 0.000020 - Loss :: 0.1333:   0%|          | 18/40000 [05:30<198:48:41, 17.90s/it]=>> [Epoch 018] Global Step 000018 =>> LR :: 0.000020 - Loss :: 0.1295:   0%|          | 18/40000 [05:30<198:48:41, 17.90s/it]=>> [Epoch 018] Global Step 000018 =>> LR :: 0.000020 - Loss :: 0.1295:   0%|          | 19/40000 [05:49<200:02:05, 18.01s/it]=>> [Epoch 019] Global Step 000019 =>> LR :: 0.000020 - Loss :: 0.1284:   0%|          | 19/40000 [05:49<200:02:05, 18.01s/it]=>> [Epoch 019] Global Step 000019 =>> LR :: 0.000020 - Loss :: 0.1284:   0%|          | 20/40000 [06:07<201:13:47, 18.12s/it]=>> [Epoch 020] Global Step 000020 =>> LR :: 0.000020 - Loss :: 0.1267:   0%|          | 20/40000 [06:07<201:13:47, 18.12s/it]slurmstepd: error: *** JOB 72416 ON dgx-034 CANCELLED AT 2025-04-22T22:32:07 ***
